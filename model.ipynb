{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Machine learning metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeline\n",
    "def process_data(df):\n",
    "    \"\"\"\n",
    "    Prepare data that can be readily consumed by ML/DL algorithms.\n",
    "    - remove elevation outliers for Spruce/Fir and Lodgepole Pine\n",
    "    - separate features from class variables\n",
    "    - split into training and testing dataset\n",
    "    - scale numerical data\n",
    "    \n",
    "    param: a dataframe of input data\n",
    "    output: X_train_normalized, X_test_normalized, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Remove elevation outliers for Spruce/Fir\n",
    "    df_drop_outliers = df.drop(df[(df['class'] == 1) & (df['Elevation'] < 2730) | (df['Elevation'] > 3538)].index)\n",
    "    # Remove elevation outliers for Lodgepole Pine\n",
    "    df_drop_outliers = df.drop(df[(df['class'] == 2) & (df['Elevation'] < 2422) | (df['Elevation'] > 3414.0)].index)\n",
    "\n",
    "    # Split data into features and labels\n",
    "    df_features = df_drop_outliers.iloc[:, :-1]\n",
    "    # We subract 1 from every class value to include 0 as a label for the softmax\n",
    "    df_labels = df_drop_outliers['class'] \n",
    "\n",
    "    # Split into train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_features, df_labels, test_size=0.2, stratify=df_labels)\n",
    "\n",
    "    # Normalise data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_normalized = scaler.fit_transform(X_train)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_normalized, X_test_normalized, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML pipeline\n",
    "def build_model(features):\n",
    "    \"\"\"\n",
    "    Build the model architecture (and compile it).\n",
    "    input: number of features\n",
    "    output: Keras model object.\n",
    "    \"\"\"    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(features.shape[1])))\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.1))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.05))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.025))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.01))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Summary report of Keras classifier:\")\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read csv\n",
    "    filename = 'cover_data.csv'\n",
    "    print(f\"Reading {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Process data\n",
    "    X_train, X_test, y_train, y_test = process_data(df)\n",
    "\n",
    "    # Build Deep Learning Model\n",
    "    model = build_model(X_train)\n",
    "\n",
    "    # Set params\n",
    "    num_epochs = 150\n",
    "    batch_size = 128\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=10)\n",
    "    \n",
    "    # Build model\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=num_epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks=[es], \n",
    "                        validation_split=0.1, \n",
    "                        verbose=1)\n",
    "\n",
    "    # Evaluate model\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test loss: {score[0]}\")\n",
    "    print(f\"Test accuracy: {score[1]}\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert predictions to discrete values\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    class_names = ['Spruce/Fir', 'Lodgepole Pine',\n",
    "                   'Ponderosa Pine', 'Cottonwood/Willow',\n",
    "                   'Aspen', 'Douglas-fir', 'Krummholz']\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cover_data.csv\n",
      "Summary report of Keras classifier:\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_93 (Dense)            (None, 256)               14080     \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 16)                1040      \n",
      "                                                                 \n",
      " dropout_74 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,408\n",
      "Trainable params: 56,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "3215/3215 [==============================] - 16s 5ms/step - loss: 0.6135 - accuracy: 0.7410 - val_loss: 0.4861 - val_accuracy: 0.7957\n",
      "Epoch 2/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.4856 - accuracy: 0.7928 - val_loss: 0.4177 - val_accuracy: 0.8263\n",
      "Epoch 3/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.4342 - accuracy: 0.8168 - val_loss: 0.3720 - val_accuracy: 0.8456\n",
      "Epoch 4/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.3994 - accuracy: 0.8321 - val_loss: 0.3418 - val_accuracy: 0.8578\n",
      "Epoch 5/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.3758 - accuracy: 0.8431 - val_loss: 0.3198 - val_accuracy: 0.8687\n",
      "Epoch 6/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.3586 - accuracy: 0.8508 - val_loss: 0.3058 - val_accuracy: 0.8750\n",
      "Epoch 7/150\n",
      "3215/3215 [==============================] - 14s 5ms/step - loss: 0.3443 - accuracy: 0.8573 - val_loss: 0.2941 - val_accuracy: 0.8812\n",
      "Epoch 8/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.3323 - accuracy: 0.8630 - val_loss: 0.2792 - val_accuracy: 0.8866\n",
      "Epoch 9/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.3220 - accuracy: 0.8672 - val_loss: 0.2715 - val_accuracy: 0.8897\n",
      "Epoch 10/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.3130 - accuracy: 0.8715 - val_loss: 0.2639 - val_accuracy: 0.8922\n",
      "Epoch 11/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.3068 - accuracy: 0.8732 - val_loss: 0.2562 - val_accuracy: 0.8951\n",
      "Epoch 12/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2993 - accuracy: 0.8767 - val_loss: 0.2504 - val_accuracy: 0.8991\n",
      "Epoch 13/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2944 - accuracy: 0.8792 - val_loss: 0.2443 - val_accuracy: 0.9011\n",
      "Epoch 14/150\n",
      "3215/3215 [==============================] - 14s 5ms/step - loss: 0.2895 - accuracy: 0.8805 - val_loss: 0.2387 - val_accuracy: 0.9036\n",
      "Epoch 15/150\n",
      "3215/3215 [==============================] - 14s 5ms/step - loss: 0.2842 - accuracy: 0.8837 - val_loss: 0.2364 - val_accuracy: 0.9037\n",
      "Epoch 16/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2812 - accuracy: 0.8845 - val_loss: 0.2320 - val_accuracy: 0.9055\n",
      "Epoch 17/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2764 - accuracy: 0.8866 - val_loss: 0.2278 - val_accuracy: 0.9074\n",
      "Epoch 18/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2739 - accuracy: 0.8872 - val_loss: 0.2228 - val_accuracy: 0.9075\n",
      "Epoch 19/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2689 - accuracy: 0.8899 - val_loss: 0.2219 - val_accuracy: 0.9100\n",
      "Epoch 20/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2667 - accuracy: 0.8903 - val_loss: 0.2187 - val_accuracy: 0.9112\n",
      "Epoch 21/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2641 - accuracy: 0.8922 - val_loss: 0.2154 - val_accuracy: 0.9132\n",
      "Epoch 22/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2619 - accuracy: 0.8921 - val_loss: 0.2155 - val_accuracy: 0.9137\n",
      "Epoch 23/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2590 - accuracy: 0.8939 - val_loss: 0.2138 - val_accuracy: 0.9120\n",
      "Epoch 24/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2567 - accuracy: 0.8947 - val_loss: 0.2100 - val_accuracy: 0.9150\n",
      "Epoch 25/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2550 - accuracy: 0.8956 - val_loss: 0.2088 - val_accuracy: 0.9155\n",
      "Epoch 26/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2517 - accuracy: 0.8965 - val_loss: 0.2030 - val_accuracy: 0.9175\n",
      "Epoch 27/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2515 - accuracy: 0.8970 - val_loss: 0.2036 - val_accuracy: 0.9174\n",
      "Epoch 28/150\n",
      "3215/3215 [==============================] - 16s 5ms/step - loss: 0.2490 - accuracy: 0.8981 - val_loss: 0.2077 - val_accuracy: 0.9154\n",
      "Epoch 29/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2481 - accuracy: 0.8986 - val_loss: 0.2014 - val_accuracy: 0.9188\n",
      "Epoch 30/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2463 - accuracy: 0.9000 - val_loss: 0.2011 - val_accuracy: 0.9174\n",
      "Epoch 31/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2437 - accuracy: 0.9007 - val_loss: 0.1964 - val_accuracy: 0.9190\n",
      "Epoch 32/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2423 - accuracy: 0.9017 - val_loss: 0.1941 - val_accuracy: 0.9215\n",
      "Epoch 33/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2417 - accuracy: 0.9012 - val_loss: 0.1959 - val_accuracy: 0.9209\n",
      "Epoch 34/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2395 - accuracy: 0.9026 - val_loss: 0.1909 - val_accuracy: 0.9223\n",
      "Epoch 35/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2392 - accuracy: 0.9021 - val_loss: 0.1948 - val_accuracy: 0.9203\n",
      "Epoch 36/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2381 - accuracy: 0.9028 - val_loss: 0.1943 - val_accuracy: 0.9213\n",
      "Epoch 37/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2361 - accuracy: 0.9033 - val_loss: 0.1917 - val_accuracy: 0.9230\n",
      "Epoch 38/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2341 - accuracy: 0.9046 - val_loss: 0.1903 - val_accuracy: 0.9235\n",
      "Epoch 39/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2342 - accuracy: 0.9042 - val_loss: 0.1892 - val_accuracy: 0.9231\n",
      "Epoch 40/150\n",
      "3215/3215 [==============================] - 14s 5ms/step - loss: 0.2333 - accuracy: 0.9049 - val_loss: 0.1872 - val_accuracy: 0.9249\n",
      "Epoch 41/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2318 - accuracy: 0.9056 - val_loss: 0.1838 - val_accuracy: 0.9255\n",
      "Epoch 42/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2316 - accuracy: 0.9055 - val_loss: 0.1857 - val_accuracy: 0.9251\n",
      "Epoch 43/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2296 - accuracy: 0.9063 - val_loss: 0.1824 - val_accuracy: 0.9263\n",
      "Epoch 44/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2292 - accuracy: 0.9067 - val_loss: 0.1839 - val_accuracy: 0.9258\n",
      "Epoch 45/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2274 - accuracy: 0.9073 - val_loss: 0.1828 - val_accuracy: 0.9264\n",
      "Epoch 46/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2271 - accuracy: 0.9078 - val_loss: 0.1804 - val_accuracy: 0.9295\n",
      "Epoch 47/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2269 - accuracy: 0.9080 - val_loss: 0.1838 - val_accuracy: 0.9256\n",
      "Epoch 48/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2257 - accuracy: 0.9081 - val_loss: 0.1801 - val_accuracy: 0.9269\n",
      "Epoch 49/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2244 - accuracy: 0.9086 - val_loss: 0.1775 - val_accuracy: 0.9301\n",
      "Epoch 50/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2238 - accuracy: 0.9088 - val_loss: 0.1798 - val_accuracy: 0.9258\n",
      "Epoch 51/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2227 - accuracy: 0.9098 - val_loss: 0.1777 - val_accuracy: 0.9284\n",
      "Epoch 52/150\n",
      "3215/3215 [==============================] - 14s 5ms/step - loss: 0.2226 - accuracy: 0.9095 - val_loss: 0.1756 - val_accuracy: 0.9293\n",
      "Epoch 53/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2219 - accuracy: 0.9097 - val_loss: 0.1818 - val_accuracy: 0.9268\n",
      "Epoch 54/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2201 - accuracy: 0.9107 - val_loss: 0.1753 - val_accuracy: 0.9297\n",
      "Epoch 55/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2200 - accuracy: 0.9108 - val_loss: 0.1754 - val_accuracy: 0.9294\n",
      "Epoch 56/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2190 - accuracy: 0.9108 - val_loss: 0.1722 - val_accuracy: 0.9308\n",
      "Epoch 57/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2187 - accuracy: 0.9113 - val_loss: 0.1743 - val_accuracy: 0.9304\n",
      "Epoch 58/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2188 - accuracy: 0.9113 - val_loss: 0.1740 - val_accuracy: 0.9296\n",
      "Epoch 59/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2182 - accuracy: 0.9115 - val_loss: 0.1740 - val_accuracy: 0.9294\n",
      "Epoch 60/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2170 - accuracy: 0.9121 - val_loss: 0.1730 - val_accuracy: 0.9303\n",
      "Epoch 61/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2167 - accuracy: 0.9122 - val_loss: 0.1708 - val_accuracy: 0.9322\n",
      "Epoch 62/150\n",
      "3215/3215 [==============================] - 16s 5ms/step - loss: 0.2164 - accuracy: 0.9121 - val_loss: 0.1707 - val_accuracy: 0.9326\n",
      "Epoch 63/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2154 - accuracy: 0.9131 - val_loss: 0.1737 - val_accuracy: 0.9312\n",
      "Epoch 64/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2142 - accuracy: 0.9129 - val_loss: 0.1690 - val_accuracy: 0.9325\n",
      "Epoch 65/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2139 - accuracy: 0.9133 - val_loss: 0.1694 - val_accuracy: 0.9324\n",
      "Epoch 66/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2132 - accuracy: 0.9135 - val_loss: 0.1718 - val_accuracy: 0.9313\n",
      "Epoch 67/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2133 - accuracy: 0.9135 - val_loss: 0.1706 - val_accuracy: 0.9325\n",
      "Epoch 68/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2124 - accuracy: 0.9138 - val_loss: 0.1688 - val_accuracy: 0.9333\n",
      "Epoch 69/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2112 - accuracy: 0.9145 - val_loss: 0.1689 - val_accuracy: 0.9326\n",
      "Epoch 70/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2119 - accuracy: 0.9145 - val_loss: 0.1672 - val_accuracy: 0.9332\n",
      "Epoch 71/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2111 - accuracy: 0.9143 - val_loss: 0.1663 - val_accuracy: 0.9343\n",
      "Epoch 72/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2105 - accuracy: 0.9147 - val_loss: 0.1670 - val_accuracy: 0.9337\n",
      "Epoch 73/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2094 - accuracy: 0.9149 - val_loss: 0.1684 - val_accuracy: 0.9319\n",
      "Epoch 74/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2104 - accuracy: 0.9142 - val_loss: 0.1657 - val_accuracy: 0.9329\n",
      "Epoch 75/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2089 - accuracy: 0.9151 - val_loss: 0.1639 - val_accuracy: 0.9348\n",
      "Epoch 76/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2091 - accuracy: 0.9152 - val_loss: 0.1637 - val_accuracy: 0.9341\n",
      "Epoch 77/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2065 - accuracy: 0.9163 - val_loss: 0.1637 - val_accuracy: 0.9349\n",
      "Epoch 78/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2080 - accuracy: 0.9156 - val_loss: 0.1651 - val_accuracy: 0.9340\n",
      "Epoch 79/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2072 - accuracy: 0.9162 - val_loss: 0.1654 - val_accuracy: 0.9340\n",
      "Epoch 80/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2073 - accuracy: 0.9161 - val_loss: 0.1649 - val_accuracy: 0.9338\n",
      "Epoch 81/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2071 - accuracy: 0.9159 - val_loss: 0.1619 - val_accuracy: 0.9362\n",
      "Epoch 82/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2073 - accuracy: 0.9160 - val_loss: 0.1651 - val_accuracy: 0.9345\n",
      "Epoch 83/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2066 - accuracy: 0.9162 - val_loss: 0.1607 - val_accuracy: 0.9359\n",
      "Epoch 84/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2053 - accuracy: 0.9173 - val_loss: 0.1631 - val_accuracy: 0.9347\n",
      "Epoch 85/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2044 - accuracy: 0.9171 - val_loss: 0.1613 - val_accuracy: 0.9357\n",
      "Epoch 86/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2041 - accuracy: 0.9177 - val_loss: 0.1617 - val_accuracy: 0.9346\n",
      "Epoch 87/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2038 - accuracy: 0.9174 - val_loss: 0.1611 - val_accuracy: 0.9359\n",
      "Epoch 88/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2038 - accuracy: 0.9175 - val_loss: 0.1625 - val_accuracy: 0.9353\n",
      "Epoch 89/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2030 - accuracy: 0.9178 - val_loss: 0.1623 - val_accuracy: 0.9341\n",
      "Epoch 90/150\n",
      "3215/3215 [==============================] - 15s 5ms/step - loss: 0.2032 - accuracy: 0.9174 - val_loss: 0.1624 - val_accuracy: 0.9356\n",
      "Epoch 91/150\n",
      "3215/3215 [==============================] - 14s 4ms/step - loss: 0.2024 - accuracy: 0.9183 - val_loss: 0.1622 - val_accuracy: 0.9358\n",
      "Test loss: 0.16193875670433044\n",
      "Test accuracy: 0.9360517859458923\n",
      "3572/3572 [==============================] - 4s 1ms/step\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "       Spruce/Fir       0.94      0.92      0.93     41519\n",
      "   Lodgepole Pine       0.94      0.96      0.95     56493\n",
      "   Ponderosa Pine       0.92      0.95      0.93      7151\n",
      "Cottonwood/Willow       0.89      0.80      0.84       549\n",
      "            Aspen       0.84      0.78      0.81      1899\n",
      "      Douglas-fir       0.90      0.88      0.89      3473\n",
      "        Krummholz       0.93      0.91      0.92      3196\n",
      "\n",
      "         accuracy                           0.94    114280\n",
      "        macro avg       0.91      0.89      0.90    114280\n",
      "     weighted avg       0.94      0.94      0.94    114280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d3ad103a38a5e5980b0a2ddf222334b9b3630c94a7e75a8e45e8afe280f469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
